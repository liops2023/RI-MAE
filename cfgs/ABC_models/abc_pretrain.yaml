# Configuration for Pretraining on ABC Dataset

optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.0005,
  weight_decay : 0.05
}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 300,  # Adjust epochs as needed for pretraining
    initial_epochs : 10
}}

# Note: ABCDataset.py needs to handle the 'subset' parameter correctly
# Ensure ROOT_DIR in abc_dataset.yaml points to the correct base path
dataset : {
  train : { _base_: cfgs/dataset_configs/abc_dataset.yaml,
            others: {subset: 'train', N_POINTS: 2048, rot_aug: True}}, # N_POINTS from abc_dataset.yaml might be overridden here
  val : { _base_: cfgs/dataset_configs/abc_dataset.yaml,            # Used for SVM validation in pretrain runner
            others: {subset: 'test', N_POINTS: 1024, rot_aug: False}}, # Use 1024 points for validation consistent with runner
  extra_train : { _base_: cfgs/dataset_configs/abc_dataset.yaml,    # Used for SVM validation in pretrain runner
            others: {subset: 'train', N_POINTS: 1024, rot_aug: False}}, # Use 1024 points for validation
}

model : {
  NAME: RITransformer_MAE,  # Assuming using the MAE variant for pretraining
  trans_dim: 384,
  depth: 12,
  drop_path_rate: 0.1,
  num_heads: 6,
  group_size: 32,
  num_group: 64,
  encoder_dims: 256,
  decoder_dims: 128, # Example decoder dim, adjust as needed for MAE
  decoder_depth: 4,  # Example decoder depth
  use_cutmix: False, # Or True, depending on pretraining strategy
  # cls_dim is not typically used directly in MAE pretraining loss
}

npoints: 2048 # Number of points used during training transforms/forward pass
total_bs : 32 # Adjust batch size based on GPU memory
step_per_update : 1
max_epoch : 300 # Adjust epochs as needed
grad_norm_clip : 10
clip_gradients : True # Consistent with runner_RIMAE_pretrain.py

# Pretraining does not optimize for a specific metric directly, validation is via SVM
# consider_metric: Not applicable for pretraining runner 