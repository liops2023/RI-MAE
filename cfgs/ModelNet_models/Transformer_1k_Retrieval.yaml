# cfgs/ModelNet_models/Transformer_1k_Retrieval.yaml

optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.0005,
  weight_decay : 0.05
}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 300,
    initial_epochs : 10
}}

dataset : {
  # Ensure the dataset provides labels suitable for both classification and retrieval
  train : { _base_: cfgs/dataset_configs/ModelNet40.yaml,
            others: {subset: 'train'}},
  val : { _base_: cfgs/dataset_configs/ModelNet40.yaml,
            others: {subset: 'test'}}, # Use test set for validation during finetuning
  test : { _base_: cfgs/dataset_configs/ModelNet40.yaml,
            others: {subset: 'test'}}} # Final testing

# Model Configuration
model : {
  NAME: RITransformer_Finetune, # Updated model name
  tasks: ['classification', 'retrieval'], # Specify tasks to run

  # Backbone config (should match RI_MAE_Base config used for pretraining)
  trans_dim: 384,
  # depth: 12, # Depth parameters are part of RI_MAE_Base, loaded with backbone
  drop_path_rate: 0.1,
  # num_heads: 6,
  # group_size: 32,
  # num_group: 64,
  # encoder_dims: 256,
  # These backbone structure parameters are defined within RI_MAE_Base
  # and should be loaded from the pretrained model or consistent config.

  # Classification specific config (only used if 'classification' in tasks)
  cls_dim: 40, # ModelNet40 has 40 classes
  cls_head_dim: 256, # Optional: configure classification head hidden dim
  cls_dropout: 0.5,  # Optional: configure classification head dropout

  # Retrieval specific config (only used if 'retrieval' in tasks)
  retrieval_config: {
    embedding_size: 512,       # Desired output embedding dimension for retrieval
    num_classes: 40,           # Number of classes in ModelNet40 (used for ProxyAnchor/PartialFC)
    loss_margin: 0.1,          # Margin for ProxyAnchorLoss
    loss_alpha: 32,            # Alpha scaling factor for ProxyAnchorLoss
    use_partial_fc: false,     # Set to true to enable Partial FC
    partial_fc_sample_rate: 0.1 # Sample rate if Partial FC is enabled (only relevant if use_partial_fc is true)
  }
}

# Optional: Define weights for multi-task loss
loss_weights: {
  classification: 1.0,
  retrieval: 1.0 # Adjust weights as needed
}

# Training settings
npoints: 1024
total_bs : 32 # Global batch size across all GPUs
step_per_update : 1
max_epoch : 300
grad_norm_clip : 10

# Pretrained backbone path (Important for finetuning)
# MODIFY THIS PATH to your RI-MAE pretrained model checkpoint
pretrained_path: '/path/to/your/ri_mae_pretrained_checkpoint.pth'

# Metric to consider for saving the best model checkpoint
# Options could be 'classification_acc', 'retrieval_recall_1', etc.
# Depending on primary goal or implement logic to track multiple best models.
consider_metric: classification_acc 